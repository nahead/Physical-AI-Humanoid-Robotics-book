"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[997],{2078:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/whisper-voice-to-action","title":"Voice-to-Action with OpenAI Whisper","description":"Natural language understanding is a critical component for intuitive human-robot interaction. Enabling robots to accurately transcribe spoken commands and then act upon them bridges the gap between human intent and robotic execution. OpenAI Whisper, a robust automatic speech recognition (ASR) system, provides an excellent foundation for converting human voice commands into text that can then be processed by Large Language Models (LLMs) and translated into robot actions.","source":"@site/docs/module4-vla/02-whisper-voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/whisper-voice-to-action","permalink":"/docs/module4-vla/whisper-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/nahead/Physical-AI-Humanoid-Robotics-book/tree/main/docs/module4-vla/02-whisper-voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Module 4: VLA","permalink":"/docs/category/module-4-vla"},"next":{"title":"Capstone Project: Autonomous Humanoid (Voice \u2192 Plan \u2192 Navigation \u2192 Vision \u2192 Manipulation)","permalink":"/docs/module4-vla/capstone-project"}}');var i=t(4848),r=t(8453);const a={},s="Voice-to-Action with OpenAI Whisper",l={},c=[{value:"Concepts",id:"concepts",level:2},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"OpenAI Whisper",id:"openai-whisper",level:3},{value:"From Text to Action",id:"from-text-to-action",level:3},{value:"Working Code Examples",id:"working-code-examples",level:2},{value:"Example 1: Transcribing Audio with OpenAI Whisper (Python)",id:"example-1-transcribing-audio-with-openai-whisper-python",level:3},{value:"<strong>Prerequisites:</strong>",id:"prerequisites",level:4},{value:"<code>whisper_transcriber.py</code>",id:"whisper_transcriberpy",level:4},{value:"Example 2: Bridging Whisper Text to LLM for Robot Action (Conceptual Python/ROS 2)",id:"example-2-bridging-whisper-text-to-llm-for-robot-action-conceptual-pythonros-2",level:3},{value:"<code>whisper_llm_robot_bridge.py</code>",id:"whisper_llm_robot_bridgepy",level:4},{value:"How to Run",id:"how-to-run",level:3},{value:"Diagrams (Text-based)",id:"diagrams-text-based",level:2},{value:"Diagram 1: Voice-to-Action Pipeline with Whisper and LLM",id:"diagram-1-voice-to-action-pipeline-with-whisper-and-llm",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,i.jsx)(n.p,{children:"Natural language understanding is a critical component for intuitive human-robot interaction. Enabling robots to accurately transcribe spoken commands and then act upon them bridges the gap between human intent and robotic execution. OpenAI Whisper, a robust automatic speech recognition (ASR) system, provides an excellent foundation for converting human voice commands into text that can then be processed by Large Language Models (LLMs) and translated into robot actions."}),"\n",(0,i.jsx)(n.h2,{id:"concepts",children:"Concepts"}),"\n",(0,i.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,i.jsx)(n.p,{children:"ASR systems convert spoken language into written text. This is the first crucial step in a voice-to-action pipeline. Key challenges for ASR in robotics include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Noise Robustness: Robots often operate in noisy environments."}),"\n",(0,i.jsx)(n.li,{children:"Accent and Dialect Variation: ASR needs to be robust to diverse speech patterns."}),"\n",(0,i.jsx)(n.li,{children:"Domain-Specific Vocabulary: Robotics often involves specialized terminology."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"OpenAI Whisper stands out for its strong performance across these challenges, having been trained on a massive and diverse dataset."}),"\n",(0,i.jsx)(n.h3,{id:"openai-whisper",children:"OpenAI Whisper"}),"\n",(0,i.jsx)(n.p,{children:"Whisper is an open-source ASR model released by OpenAI. It can transcribe audio into text in multiple languages and even translate spoken language into English. Its key features include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"High Accuracy: Achieves state-of-the-art performance on various benchmarks."}),"\n",(0,i.jsx)(n.li,{children:"Multilingual: Supports transcription in numerous languages."}),"\n",(0,i.jsx)(n.li,{children:"Robustness: Performs well even with background noise or varied speaking styles."}),"\n",(0,i.jsx)(n.li,{children:"Open Source: Available for free use, allowing for local deployment or cloud-based API access."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"from-text-to-action",children:"From Text to Action"}),"\n",(0,i.jsx)(n.p,{children:"Once a voice command is transcribed into text by Whisper, the next steps in the VLA pipeline typically involve:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Large Language Model (LLM) Processing: The transcribed text is fed to an LLM (e.g., GPT-4, Llama 2). The LLM's role is to:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Interpret Intent: Understand the underlying goal of the human command."}),"\n",(0,i.jsx)(n.li,{children:"Disambiguate: Ask clarifying questions if the command is vague."}),"\n",(0,i.jsx)(n.li,{children:"Decompose Task: Break down complex commands into a sequence of simpler, actionable sub-tasks."}),"\n",(0,i.jsx)(n.li,{children:"Generate Robot-Executable Commands: Output a structured representation of actions that the robot's control system can understand (e.g., a JSON object defining a navigation goal, a manipulation command)."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Action Pipeline: The LLM's output is then translated into ROS 2 messages, services, or action goals that trigger the robot's low-level controllers."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"working-code-examples",children:"Working Code Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-transcribing-audio-with-openai-whisper-python",children:"Example 1: Transcribing Audio with OpenAI Whisper (Python)"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates how to use the OpenAI Whisper model (local or via API) to transcribe an audio file."}),"\n",(0,i.jsx)(n.h4,{id:"prerequisites",children:(0,i.jsx)(n.strong,{children:"Prerequisites:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Install ",(0,i.jsx)(n.code,{children:"openai"})," Python package: ",(0,i.jsx)(n.code,{children:"pip install openai"})]}),"\n",(0,i.jsxs)(n.li,{children:["For local Whisper models, install ",(0,i.jsx)(n.code,{children:"whisper"})," Python package: ",(0,i.jsx)(n.code,{children:"pip install -U openai-whisper"})]}),"\n",(0,i.jsxs)(n.li,{children:["For local models, you also need ",(0,i.jsx)(n.code,{children:"ffmpeg"})," installed on your system."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"whisper_transcriberpy",children:(0,i.jsx)(n.code,{children:"whisper_transcriber.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import openai\nimport os\nimport io\nimport wave\nimport numpy as np\n\n# Option 1: Using OpenAI API (Requires API key)\n# openai.api_key = os.getenv("OPENAI_API_KEY")\n\ndef transcribe_audio_api(audio_file_path):\n    """Transcribes an audio file using OpenAI Whisper API."""\n    try:\n        with open(audio_file_path, "rb") as audio_file:\n            transcript = openai.audio.transcriptions.create(\n                model="whisper-1", \n                file=audio_file, \n                response_format="text"\n            )\n            return transcript\n    except Exception as e:\n        print(f"Error during API transcription: {e}")\n        return None\n\n# Option 2: Using local Whisper model (CPU/GPU)\ntry:\n    import whisper\n    # Load a smaller model for faster inference if needed (tiny, base, small, medium, large)\n    local_model = whisper.load_model("base") \nexcept ImportError:\n    local_model = None\n    print("whisper Python package not found. Install with \'pip install openai-whisper\' for local transcription.")\n\n\ndef transcribe_audio_local(audio_file_path):\n    """Transcribes an audio file using a local Whisper model."""\n    if local_model is None:\n        return "Local Whisper model not available. Cannot transcribe locally."\n    try:\n        result = local_model.transcribe(audio_file_path)\n        return result["text"]\n    except Exception as e:\n        print(f"Error during local transcription: {e}")\n        return None\n\n# Example Usage\nif __name__ == "__main__":\n    # Create a dummy audio file for testing\n    dummy_audio_path = "dummy_audio.wav"\n    with wave.open(dummy_audio_path, \'w\') as wf:\n        wf.setnchannels(1) # mono\n        wf.setsampwidth(2) # 16-bit\n        wf.setframerate(16000) # 16kHz\n        # Generate a simple tone\n        frequency = 440  # Hz\n        duration = 1  # seconds\n        amplitude = 32767 # Max 16-bit amplitude\n        t = np.linspace(0, duration, int(16000 * duration), endpoint=False)\n        data = amplitude * np.sin(2 * np.pi * frequency * t)\n        wf.writeframes(data.astype(np.int16).tobytes())\n\n    print(f"Dummy audio file created at {dummy_audio_path}")\n\n    # Use local transcription if available\n    if local_model:\n        print("\\n--- Local Whisper Transcription ---")\n        transcription_local = transcribe_audio_local(dummy_audio_path)\n        print(f"Transcription: {transcription_local}")\n    else:\n        print("\\n--- Local Whisper not configured ---")\n\n    # Use API transcription if API key is set\n    # if openai.api_key:\n    #     print("\\n--- OpenAI API Whisper Transcription ---")\n    #     transcription_api = transcribe_audio_api(dummy_audio_path)\n    #     print(f"Transcription: {transcription_api}")\n    # else:\n    #     print("\\n--- OPENAI_API_KEY environment variable not set. Skipping API transcription. ---")\n\n    os.remove(dummy_audio_path)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-bridging-whisper-text-to-llm-for-robot-action-conceptual-pythonros-2",children:"Example 2: Bridging Whisper Text to LLM for Robot Action (Conceptual Python/ROS 2)"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates how to pass the transcribed text from Whisper to an LLM and then interpret the LLM's response to trigger a ROS 2 action."}),"\n",(0,i.jsx)(n.h4,{id:"whisper_llm_robot_bridgepy",children:(0,i.jsx)(n.code,{children:"whisper_llm_robot_bridge.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport openai # Assuming API or a wrapper for local LLM\n\n# --- ROS 2 Action Message (Conceptual, replace with actual) ---\n# For example, a custom action for \'NavigateToPose\'\n# class NavigateToPose_SendGoal(Action):\n#     target_pose: geometry_msgs.msg.PoseStamped\n# --- End of Conceptual ROS 2 Action Message ---\n\nclass WhisperLLMRobotBridge(Node):\n    def __init__(self):\n        super().__init__(\'whisper_llm_robot_bridge\')\n        self.transcribed_text_sub = self.create_subscription(\n            String,\n            \'whisper_text_output\', # Topic where Whisper publishes transcribed text\n            self.text_callback,\n            10)\n        self.get_logger().info(\'WhisperLLMRobotBridge node started. Waiting for transcribed text...\')\n\n        # --- Conceptual LLM Client ---\n        # Assuming an OpenAI client or a custom service client to your local LLM\n        # self.llm_client = openai.OpenAI() # For OpenAI API\n        # Or self.llm_service_client = self.create_client(LLMQuery, \'llm_inference_service\')\n        # --- End Conceptual LLM Client ---\n        self.llm_response_pub = self.create_publisher(String, \'llm_robot_commands\', 10) # For robot commands\n\n    def text_callback(self, msg):\n        transcribed_text = msg.data\n        self.get_logger().info(f"Received transcribed text: \'{transcribed_text}\'")\n\n        # --- Step 1: Send transcribed text to LLM ---\n        # Conceptual LLM Prompt\n        prompt = f"""\nThe user said: "{transcribed_text}"\nBased on this, what is the most appropriate robot command?\nRespond with a JSON object containing \'command\' and \'parameters\'.\nExample: {{"command": "navigate_to_pose", "parameters": {{"x": 1.0, "y": 0.5, "theta": 0.0}}}}\nExample: {{"command": "pick_object", "parameters": {{"object_id": "red_block", "location": "table"}}}}\nExample: {{"command": "say_response", "parameters": {{"text": "I did not understand. Can you rephrase?"}}}}\n"""\n        \n        try:\n            # --- Conceptual LLM Call ---\n            # response = self.llm_client.chat.completions.create(\n            #     model="gpt-3.5-turbo",\n            #     messages=[{"role": "user", "content": prompt}]\n            # )\n            # llm_output_text = response.choices[0].message.content\n            # For demonstration, simulate LLM response:\n            if "go forward" in transcribed_text.lower():\n                llm_output_text = \'{{"command": "navigate_forward", "parameters": {{"distance": 1.0}}}}\'\n            elif "pick up the block" in transcribed_text.lower():\n                llm_output_text = \'{{"command": "pick_object", "parameters": {{"object_id": "block"}}}}\'\n            else:\n                llm_output_text = \'{{"command": "say_response", "parameters": {{"text": "I did not understand. Can you rephrase?"}}}}\'\n            # --- End Conceptual LLM Call ---\n\n            self.get_logger().info(f"LLM generated command: {llm_output_text}")\n            \n            # --- Step 2: Parse LLM response and trigger ROS 2 action ---\n            robot_command = json.loads(llm_output_text)\n            self._execute_robot_command(robot_command)\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"LLM response was not valid JSON: {e}")\n        except Exception as e:\n            self.get_logger().error(f"Error processing LLM command: {e}")\n\n    def _execute_robot_command(self, command_data):\n        command_type = command_data.get("command")\n        parameters = command_data.get("parameters", {})\n\n        if command_type == "navigate_forward":\n            self.get_logger().info(f"Triggering navigation action: forward by {parameters.get(\'distance\')}m")\n            # --- Conceptual ROS 2 Action Client Call ---\n            # goal_msg = NavigateToPose_SendGoal.Goal()\n            # goal_msg.target_pose.pose.position.x = parameters.get(\'distance\')\n            # self.nav_action_client.send_goal_async(goal_msg)\n            # --- End Conceptual ROS 2 Action Client Call ---\n        elif command_type == "pick_object":\n            self.get_logger().info(f"Triggering manipulation action: pick {parameters.get(\'object_id\')}")\n            # --- Conceptual ROS 2 Action Client Call ---\n            # goal_msg = PickAndPlace_SendGoal.Goal()\n            # goal_msg.object_id = parameters.get(\'object_id\')\n            # self.manipulation_action_client.send_goal_async(goal_msg)\n            # --- End Conceptual ROS 2 Action Client Call ---\n        elif command_type == "say_response":\n            self.get_logger().info(f"Robot says: {parameters.get(\'text\')}")\n            # --- Conceptual Text-to-Speech (TTS) call ---\n            # self.tts_publisher.publish(String(data=parameters.get(\'text\')))\n            # --- End Conceptual TTS Call ---\n        else:\n            self.get_logger().warn(f"Unknown robot command: {command_type}")\n        \n        # Publish the command for other nodes to execute\n        msg = String()\n        msg.data = json.dumps(command_data)\n        self.llm_response_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperLLMRobotBridge()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'WhisperLLMRobotBridge node stopped cleanly.\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"how-to-run",children:"How to Run"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set up OpenAI API Key (if using API):"})," Set ",(0,i.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Input:"})," You'll need to feed audio to Whisper. For testing, you can use pre-recorded audio files. For real-time, you'd integrate with a microphone."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Topic for Whisper Output:"})," A separate node (or a modified version of ",(0,i.jsx)(n.code,{children:"whisper_transcriber.py"}),") would need to publish the transcribed text to the ",(0,i.jsx)(n.code,{children:"whisper_text_output"})," ROS 2 topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulate LLM and Robot Actions:"})," For ",(0,i.jsx)(n.code,{children:"whisper_llm_robot_bridge.py"}),", if you're not using the actual OpenAI API, the provided code includes a simple ",(0,i.jsx)(n.code,{children:"if/elif"})," structure to simulate LLM responses for basic commands. For robot actions, you would typically have actual ROS 2 action servers or service servers that subscribe to ",(0,i.jsx)(n.code,{children:"/llm_robot_commands"})," or are called directly by the bridge node."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Build and Source ROS 2 Workspace:"})," (refer to Module 1 for setup)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Run the Bridge Node:"})," ",(0,i.jsx)(n.code,{children:"ros2 run <your_package_name> whisper_llm_robot_bridge"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Publish Test Text:"})," In another terminal, simulate Whisper output:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub --once /whisper_text_output std_msgs/msg/String \"data: 'go forward one meter'\"\n"})}),"\n","Observe the bridge node's interpretation and conceptual action."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"diagrams-text-based",children:"Diagrams (Text-based)"}),"\n",(0,i.jsx)(n.h3,{id:"diagram-1-voice-to-action-pipeline-with-whisper-and-llm",children:"Diagram 1: Voice-to-Action Pipeline with Whisper and LLM"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"+----------------+       +-----------------------+       +-----------------------+       +---------------------+\n| Human Voice    |-----\x3e|  Microphone /         |-----\x3e|  OpenAI Whisper       |-----\x3e|  Large Language     |\n|  Command       |       |  Audio Input          |       |  (ASR)                |       |  Model (LLM)        |\n+----------------+       +-----------------------+       |                       |       | (Intent, Planning)  |\n                                                         |  (Audio to Text)      |       +----------+----------+\n                                                         +-----------------------+                  |\n                                                                                                    | (Structured Robot Command)\n                                                                                                    V\n                                                             +---------------------+\n                                                             |  ROS 2 Action       |\n                                                             |  Pipeline /         |\n                                                             |  Robot Controllers  |\n                                                             +---------------------+\n"})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Whisper Integration:"})," Integrate a microphone input with the Whisper transcription. Create a ROS 2 node that continuously listens to microphone audio, transcribes it using Whisper (local or API), and publishes the text to a ROS 2 topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enhanced LLM Prompting:"})," Experiment with different LLM prompts to improve the interpretation of complex or ambiguous commands. How can you guide the LLM to output more reliable structured robot commands? (Hint: The LLM could ask for clarification)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement a Simple Action Server:"}),' Create a dummy ROS 2 action server for a "navigate_forward" action. Connect it to the ',(0,i.jsx)(n.code,{children:"whisper_llm_robot_bridge.py"})," so that a successful navigation command triggers the dummy action."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter delved into the crucial initial step of a VLA pipeline: converting human voice commands into actionable text. You've learned about the power of OpenAI Whisper for accurate speech-to-text transcription and how its output can be fed into an LLM for intent interpretation and task decomposition. The conceptual code examples demonstrated the flow from transcribed text to structured robot commands, highlighting the integration points with ROS 2 action pipelines. This foundation in voice-to-action is essential for building intuitive and natural human-robot interfaces, bringing us closer to truly conversational robotics."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);